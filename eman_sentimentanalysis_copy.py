# -*- coding: utf-8 -*-
"""Eman_SentimentAnalysis_copy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eXajO3cxNCkw49GaITIp15dTIoaQJ5ua
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'amazon-fine-food-reviews:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F18%2F2157%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240526%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240526T123929Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8fb7cf5d16860aba3aae166cf64ad91fb5267ffba8c04e6d37c1bbf1262c0e2567ff7db51c790afa623c5556192afdbe62973b4452824b00325d3027bcdc5b9b5a7f000f6862d28d7615831153e8eb1a48c9221873ed7f7aa47be9515290474e2489b6b437d0b3cce3b481437e067bf7768d7628c2854d1c397d2d79c8bcf51fda4203ec1fda04598b1f549a07f425f29e06d6c998842ed29e7f9c6a7e7738be0c26edd23d43b05e6c16a54e74013b6a80ebd33b6b06d9890a973402f4520b6b7b72f2e911cbf2feff4d2684629069c205109b22dc6ef7459169946db78696b0a1429c48e5b8190022857b7eeb11c6214de4082b50381e90f8b5664f2fb93958,emanfile:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5076331%2F8505005%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240526%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240526T123929Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2019b12b8701cd5fe9c8bb8693e9623aedb347385de2504ebd578293552c3f3b4ebe86d12a48ef0a85a2244188ae508233444726d4f20f441b32030d2be4ee076513f0950ac39e9460e8c6f3064bf351543cdd6c8dc4a61a6fc8e7c8e828438e085aa4c682e90290038f7b71a4901f9901c758753220b125c25e1a44cb7f36b416a69ec3e4aecb1c7b30e6c287135c9e7bf02bb504c9bd3d97f3bbe06b5035bee36248a0c8385c8c1a87f91e016f71e99a0ac612c1dd0c54e582b9ba034ca773623e68f6f3684d0a1c386a8584f89cdb1fd3c0ccd00ce99ee806669cb3926bd1b2f8e31e7b0a8df89297a190ea9121ece75cdbdf3c177000e206c97ea6bea598'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)


import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

data =pd.read_csv('/kaggle/input/amazon-fine-food-reviews/Reviews.csv')

data.head()



data['Text'].values[0]

data.shape

df= data.head(500)
df.shape

df.head()



""" **Quick EDA**"""

import matplotlib.pyplot as plt
ax=df['Score'].value_counts().sort_index().plot(kind='bar',title='Count of Reviews by Stars'
,figsize=(10,5))
ax.set_xlabel('Review Stars')
plt.show()



"""**Bisic NLTK**"""

ex=df['Text'][50]
ex

import nltk

nltk.download('punkt')
tokens= nltk.word_tokenize(ex)
tokens[0:10]

nltk.download('averaged_perceptron_tagger')

tagged=nltk.pos_tag(tokens)
tagged[0:10]

nltk.download('maxent_ne_chunker')
nltk.download('words')

entities= nltk.chunk.ne_chunk(tagged)
entities.pprint()

"""**Step 1 VADER Seniment Scoring**"""

from nltk.sentiment import SentimentIntensityAnalyzer
from tqdm.notebook import tqdm
nltk.download('vader_lexicon')
sia = SentimentIntensityAnalyzer()

sia.polarity_scores('I am so happy')

sia.polarity_scores('This is the worst thing ever.')

sia.polarity_scores(ex)

#Run the Polarity scores on entire Dataset

res={}
for i, row in tqdm (df.iterrows() , total=len(df)):
    text=row['Text']
    myid=row['Id']
    res[myid]= sia.polarity_scores(text)

res

vaders= pd.DataFrame(res).T
vaders=vaders.reset_index().rename(columns={'index': 'Id'}) #merge new column
vaders=vaders.merge(df,how='left',on='Id')

vaders

# we have Sentiment Score and metadata
vaders.head()

"""**PLOT VADER Results**"""

import seaborn as sns
ax= sns.barplot(data=vaders,x='Score', y='compound')
ax.set_title('Compound Score by Amazon Star Review')
plt.show()

fig,axs=plt.subplots(1,3,figsize=(12,3))
sns.barplot(data=vaders,x='Score', y='pos' , ax=axs[0])
sns.barplot(data=vaders,x='Score', y='neu' , ax=axs[1])
sns.barplot(data=vaders,x='Score', y='neg' , ax=axs[2])
axs[0].set_title('Positive')
axs[1].set_title('Neutral')
axs[2].set_title('Negative')
plt.tight_layout()
plt.show()

"""**Step 3 Roberta Pretraind Model**"""

from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax

def preprocess(text):
    new_text = []
    for t in text.split(" "):
        t = '@user' if t.startswith('@') and len(t) > 1 else t
        t = 'http' if t.startswith('http') else t
        new_text.append(t)
    return " ".join(new_text)

from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer

MODEL = f"cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)

print(ex)
sia.polarity_scores(ex)

# Run for Roberta Model
encoded_text = tokenizer(ex, return_tensors='pt')
output = model(**encoded_text)
scores = output[0][0].detach().numpy()
scores = softmax(scores)
scores_dict = {
    'roberta_neg' : scores[0],
    'roberta_neu' : scores[1],
    'roberta_pos' : scores[2]
}
print(scores_dict)

def polarity_scores_roberta(ex):
    encoded_text = tokenizer(ex, return_tensors='pt')
    output = model(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[0],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[2]
    }
    return scores_dict

res = {}
for i, row in tqdm(df.iterrows(), total=len(df)):
    try:
        text = row['Text']
        myid = row['Id']
        vader_result = sia.polarity_scores(text)
        vader_result_rename = {}
        for key, value in vader_result.items():
            vader_result_rename[f"vader_{key}"] = value
        roberta_result = polarity_scores_roberta(text)
        both = {**vader_result_rename, **roberta_result}
        res[myid] = both
    except RuntimeError:
        print(f'Broke for id {myid}')

results_df = pd.DataFrame(res).T
results_df = results_df.reset_index().rename(columns={'index': 'Id'})
results_df = results_df.merge(df, how='left')

results_df.columns

"""# Step 3. Combine and compare"""

sns.pairplot(data=results_df,
             vars=['vader_neg', 'vader_neu', 'vader_pos',
                  'roberta_neg', 'roberta_neu', 'roberta_pos'],
            hue='Score',
            palette='tab10')
plt.show()



"""# The Transformers Pipeline"""

from transformers import pipeline
sent_pipeline = pipeline("sentiment-analysis")

sent_pipeline('I love sentiment analysis!')

sent_pipeline('Make sure to like and subscribe!')

sent_pipeline('booo')

sent_pipeline('This restaurant serves the best pizza in town, the pasta is amazing, and the service is excellent!')



